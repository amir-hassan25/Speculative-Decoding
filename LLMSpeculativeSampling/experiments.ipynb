{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Notebook\n",
    "\n",
    "## Model configurations: \n",
    "- $M_p: $ llama2 70b, $M_q: $ llamma-2 70b\n",
    "- $M_p: $ llama2 70b, $M_q: $ llamma-2 7b\n",
    "- $M_p: $ llama2 7b, $M_q: $ llamma-2 7b\n",
    "- $M_p: $ llama 30b, $M_q: $ llamma 30b\n",
    "- $M_p: $ llama 30b, $M_q: $ llamma 7b\n",
    "- $M_p: $ llama 1b, $M_q: $ llamma 1b\n",
    "- $M_p: $ bloom 7b, $M_q: $ bloom 7b\n",
    "- $M_p: $ bloom 7b, $M_q: $ bloom 560m\n",
    "- $M_p: $ bloom 560b, $M_q: $ bloom 560m\n",
    "- $M_p: $ baichuan 13b, $M_q: $ baichuan 13b\n",
    "- $M_p: $ baichuan 13b, $M_q: $ baichuan 7b\n",
    "- $M_p: $ baichuan 7b, $M_q: $ baichuan 7b\n",
    "\n",
    "## Experiment Environment\n",
    "- Single node with 8 NVIDIA RTX A6000 GPUs (48GB RAM)\n",
    "\n",
    "## Tasks: \n",
    "- Token generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # \"llama1b\": \"/share_nfs/fangjiarui/root/code/hf_models/TinyLlama-1.1B-step-50K-105b\",\n",
    "    # \"llama7b\": \"/share_nfs/tianzhi/code/llama-7b\",\n",
    "    # \"llama30b\": \"/share_nfs/fangjiarui/root/code/hf_models/llama-30b-hf\",\n",
    "    # \"llama2-7b\" : \"/share_nfs/fangjiarui/root/code/hf_models/llama-2-7b-hf\",\n",
    "    # \"llama2-70b\" : \"/share_nfs/fangjiarui/root/code/hf_models/llama-2-70b-hf\",\n",
    "    # \"bloom-560m\": \"/share_nfs/fangjiarui/root/code/hf_models/bloom-560m\",\n",
    "    # \"bloom7b\": \"/share_nfs/fangjiarui/root/code/hf_models/bloomz-7b1\",\n",
    "    # \"baichuan-7b\": \"/share_nfs/duanqiyuan/models/source_models/hf/baichuan-7B\",\n",
    "    # \"baichuan-13b\": \"/share_nfs/duanqiyuan/models/source_models/hf/Baichuan-13B-Base\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gamma vs Alpha over all model configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gamma vs Tokens Generated per Second \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GPU Parallelization vs No GPU Parallelization Measuring Generatived per Second (gamma = 7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
